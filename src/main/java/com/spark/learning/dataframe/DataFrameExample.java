package com.spark.learning.dataframe;

import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;

public class DataFrameExample {
    public static void main(String[] args) {
        // 1Ô∏è‚É£ Create SparkSession
        SparkSession spark = SparkSession.builder()
                .appName("DataFrameExample")
                .master("local[*]")
                .getOrCreate();

        // 2Ô∏è‚É£ Load JSON file into DataFrame
        Dataset<Row> df = spark.read()
                .option("multiline", "true") // For multi-line JSON files
                .json("src/main/resources/people.json");

        // 3Ô∏è‚É£ Show DataFrame content
        System.out.println("üîπ Full DataFrame:");
        df.show();

        // 4Ô∏è‚É£ Print Schema
        System.out.println("üîπ Schema:");
        df.printSchema();

        /**
         * üîç 1. How Does DataFrame Identify Schema?
         *
         * Schema = column names + data types.
         *
         * When you create a DataFrame (e.g., from JSON, CSV, Parquet):
         * Spark parses a sample of data (or all data if sampling disabled).
         * Infers data types for each field (string, int, double, boolean).
         *
         * Builds a StructType schema internally.
         * Example:
         *
         * {"name": "Alice", "age": 30}
         * {"name": "Bob", "age": 25}
         *
         *
         * Spark reads:
         * "name" ‚Üí seen as string
         * "age" ‚Üí seen as number (integer)
         *
         * Resulting schema:
         * root
         *  |-- name: string (nullable = true)
         *  |-- age: long (nullable = true)
         *
         *
         * If you don‚Äôt want inference, you can define schema manually:
         *
         * StructType schema = new StructType()
         *      .add("name", DataTypes.StringType)
         *      .add("age", DataTypes.IntegerType);
         * Dataset<Row> df = spark.read().schema(schema).json("people.json");
         */

        // 5Ô∏è‚É£ Select specific columns
        System.out.println("üîπ Select name column:");
        df.select("name").show();

        // 6Ô∏è‚É£ Filter rows where age > 25
        System.out.println("üîπ People older than 25:");
        df.filter(df.col("age").gt(25)).show();

        // 7Ô∏è‚É£ Group by age and count
        System.out.println("üîπ Count by age:");
        df.groupBy("age").count().show();

        // 8Ô∏è‚É£ Using SQL queries
        df.createOrReplaceTempView("people");
        Dataset<Row> sqlResult = spark.sql("SELECT name, age FROM people WHERE age >= 30");
        System.out.println("üîπ SQL Query Result:");
        sqlResult.show();

        spark.stop();
    }
}
